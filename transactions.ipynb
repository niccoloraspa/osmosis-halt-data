{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grequests\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import pandas as pd\n",
    "\n",
    "API_ENDPOINT_TXS = \"https://api.osmosis.interbloc.org/cosmos/tx/v1beta1/txs?events=tx.height={}&pagination.offset={}\"\n",
    "# RPC_ENDPOINT = \"https://rpc.osmosis.interbloc.org:443\" ## note: history on this node is pruned so it cannot fetch data at heights around `UPGRADE_HEIGHT`\n",
    "BLOCK_TX_FILE_PATH = \"data/block_tx/{}.json\"\n",
    "TX_RESULT_LIMIT = 100\n",
    "\n",
    "UPGRADE_HEIGHT = 4707300\n",
    "HALT_HEIGHT = 4713064\n",
    "\n",
    "GAMM_MSG_PREFIX = \"/osmosis.gamm.v1beta1\"\n",
    "UNPOOL_WHITELISTED_MSG = \"/osmosis.superfluid.MsgUnPoolWhitelistedPool\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Download block data \n",
    "\n",
    "Download all tx responses for every block in [`UPGRADE_HEIGHT`, `HALT_HEIGHT`] to `data/block_tx/`\n",
    "\n",
    "\n",
    "### Method 1. Get the downloaded data\n",
    "\n",
    "Downloaded data already avaialble [here](fixme://) (**NOTE: UDPATE LINK!**)\n",
    "\n",
    "Extract the archive in `data/block_tx/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2. Download data in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FIXME! Account for pagination (see slow version - results are grouped by pages of 100 entries, seems to be unchangeable, using workaround with 'pagination.offset')\n",
    " \n",
    "STEP = 5\n",
    "\n",
    "for height in range(UPGRADE_HEIGHT, HALT_HEIGHT+1, STEP):\n",
    "\n",
    "    urls = [API_ENDPOINT_TXS.format(h) for h in range(height, height + STEP) if not os.path.exists(BLOCK_TX_FILE_PATH.format(h)) and h <= HALT_HEIGHT]\n",
    "    rs = (grequests.get(url, headers={\"Accept\": \"application/json\"}) for url in urls)\n",
    "    responses = grequests.map(rs, size = STEP)\n",
    "\n",
    "    for idx, response in enumerate(responses):\n",
    "        curr_height = response.request.url.replace(API_ENDPOINT_TXS.format(\"\"), \"\")\n",
    "\n",
    "        with open(BLOCK_TX_FILE_PATH.format(curr_height), \"w\") as f:\n",
    "            json.dump(response.json()[\"tx_responses\"], f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3. Slow serialized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for height in range(UPGRADE_HEIGHT, HALT_HEIGHT+1):\n",
    "    \n",
    "    print(height, end=\"\\r\")\n",
    "    \n",
    "    if not os.path.exists(BLOCK_TX_FILE_PATH.format(height)):\n",
    "        offset = 0\n",
    "        response = requests.get(API_ENDPOINT_TXS.format(height, offset), headers={\"Accept\": \"application/json\"})\n",
    "        response_json = response.json()\n",
    "        total = int(response_json[\"pagination\"][\"total\"])\n",
    "        tx_responses = response_json[\"tx_responses\"]\n",
    "        while total > TX_RESULT_LIMIT:\n",
    "            total -= TX_RESULT_LIMIT\n",
    "            offset += TX_RESULT_LIMIT\n",
    "            response = requests.get(API_ENDPOINT_TXS.format(height, offset), headers={\"Accept\": \"application/json\"})\n",
    "            tx_responses.extend(response.json()[\"tx_responses\"])\n",
    "        with open(BLOCK_TX_FILE_PATH.format(height), \"w\") as f:\n",
    "                json.dump(tx_responses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load raw txs into pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1. Import the downloaded data and save DataFrame to `raw_tx.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "def filter_txs_data(dict):\n",
    "    filtered_dict = {key: dict[key] for key in [\"height\", \"txhash\", \"code\", \"timestamp\", \"tx\", \"logs\"]}\n",
    "    return filtered_dict\n",
    "\n",
    "\n",
    "for height in range(UPGRADE_HEIGHT, HALT_HEIGHT+1):\n",
    "    \n",
    "    print(\"processing block:\", height, \"remaining:\", \"{:5d}\".format(HALT_HEIGHT - height), end='\\r')\n",
    "    \n",
    "    with open(BLOCK_TX_FILE_PATH.format(height)) as f:\n",
    "        txs_data = json.load(f)\n",
    "\n",
    "        if txs_data:\n",
    "            txs = list(filter(filter_txs_data, txs_data)) \n",
    "            results += txs\n",
    "\n",
    "print(\"\\ncreating dataframe...\")\n",
    "\n",
    "raw_df = pd.DataFrame.from_records(results)\n",
    "raw_df.to_csv(\"csv/raw_txs.csv\", index=False)\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2. Load DataFrame from `raw_txs.csv` (if previously generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "raw_df = pd.read_csv(\"csv/raw_txs.csv\")\n",
    "raw_df.tx = raw_df.tx.apply(ast.literal_eval)\n",
    "raw_df.logs = raw_df.logs.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process txs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unsuccessful transactions\n",
    "df = df[df[\"code\"] == 0]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Remove unused columns\n",
    "df.drop(columns=[\"raw_log\", \"events\", \"data\", \"info\", \"codespace\", \"gas_wanted\", \"gas_used\", \"code\"], inplace=True)\n",
    "\n",
    "# Expand txs (with logs)\n",
    "df = df.join(pd.json_normalize(df.tx)[[\"body.messages\"]]).drop(columns=[\"tx\"])\n",
    "df = df.explode(column=[\"body.messages\", \"logs\"])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Extract events from logs\n",
    "df = df.join(pd.json_normalize(df.logs)[[\"events\"]]).drop(columns=[\"logs\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand messages (we just need type and sender, execution data will come from events)\n",
    "df = df.join(pd.json_normalize(df[\"body.messages\"])[[\"@type\", \"sender\"]])\n",
    "df.drop(columns=[\"body.messages\"], inplace=True)\n",
    "\n",
    "# Filter messages\n",
    "df = df[(df[\"@type\"].str.contains(GAMM_MSG_PREFIX)) | (df[\"@type\"] == UNPOOL_WHITELISTED_MSG)]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define routine to extract data from events\n",
    "def process_msg_events(row):\n",
    "    events = row[3]\n",
    "    msg_type = row[4]\n",
    "    if \"MsgSwapExactAmountIn\" in msg_type or \"MsgSwapExactAmountOut\" in msg_type:\n",
    "        for event in events:\n",
    "            if event[\"type\"] == \"token_swapped\":\n",
    "                pool_id_list = list()\n",
    "                tokens_in_list = list()\n",
    "                tokens_out_list = list()\n",
    "                swaps = zip(*(iter(event[\"attributes\"]),) * 5) # 5 entries per single \"token_swapped\" event\n",
    "                for swap in swaps:\n",
    "                    pool_id_list.append(swap[2][\"value\"])\n",
    "                    tokens_in_list.append(swap[3][\"value\"])\n",
    "                    tokens_out_list.append(swap[4][\"value\"])\n",
    "            return pool_id_list, tokens_in_list, tokens_out_list\n",
    "    elif \"MsgJoinPool\" in msg_type or \"MsgJoinSwapExternAmountIn\" in msg_type or \"MsgJoinSwapShareAmountOut\" in msg_type:\n",
    "        for event in events:\n",
    "            if event[\"type\"] == \"pool_joined\":\n",
    "                pool_id = event[\"attributes\"][2][\"value\"]\n",
    "                tokens_in = event[\"attributes\"][3][\"value\"]\n",
    "            elif event[\"type\"] == \"coinbase\":\n",
    "                shares_minted = event[\"attributes\"][1][\"value\"]\n",
    "        return pool_id, tokens_in, shares_minted\n",
    "    elif \"MsgExitPool\" in msg_type or \"MsgExitSwapExternAmountOut\" in msg_type or \"MsgExitSwapShareAmountIn\" in msg_type or \"MsgUnPoolWhitelistedPool\" in msg_type:\n",
    "        for event in events:\n",
    "            if event[\"type\"] == \"pool_exited\":\n",
    "                pool_id = event[\"attributes\"][2][\"value\"]\n",
    "                tokens_out = event[\"attributes\"][3][\"value\"]\n",
    "            elif event[\"type\"] == \"burn\":\n",
    "                shares_burned = event[\"attributes\"][1][\"value\"]\n",
    "        return pool_id, shares_burned, tokens_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from events\n",
    "df[\"poolId\"], df[\"tokensIn\"], df[\"tokensOut\"] = zip(*df.apply(process_msg_events, axis=1))\n",
    "df.drop(columns=[\"events\"], inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split swaps into separate rows (one row per poolId)\n",
    "df = df.explode(column=[\"poolId\", \"tokensIn\", \"tokensOut\"])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save processed txs as `txs.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"csv/txs.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the processed DataFrame contains the ordered sequence of `gamm` transactions with the relevant execution data (plus `MsgUnPoolWhitelistedPool` as it removes liquidity from pools, this is marginal and only implemented to perform consistency checks). \n",
    "We can now simulate running all transactions on the various pools, sequentially, starting from the state of pools at `UPGRADE_HEIGHT-1`.\n",
    "\n",
    "For any transaction except `MsgJoinPool`, we can use the execution data to update the pools state *as is* (the bug was only present in `MaximalExactRatioJoin()` which can only be reached if `tokensIn.len() != 1` in `calcJoinPoolShares()`, only possible via a `MsgJoinPool`).\n",
    "\n",
    "When a `MsgJoinPool` is encountered, we also compute the **correct** number of shares. Seemingly, exit pool transactions require also a recalculation of liquidity removed using corrected history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-load previously saved `txs.csv`\n",
    "\n",
    "skip to this part when reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"csv/txs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find wallets affected and their credit/debt towards the protocol\n",
    "\n",
    "In this analysis, the debit accrued due to exploitation will be denominated in all assets exited from the pool(s), not accounting for successive swaps/transfers that finalize the exploit. The attack pattern usually ends up with a final swap that consolidates the gains to a single asset, but this final calculation can be done separately (and is likely already done somewhere by someone, at least for the bigger attackers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define functions to query node for onchain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to query onchain pool and address balance data at various heights\n",
    "# credit to @george-aj (https://github.com/george-aj/osmosis-nitrogen-extra-gamm-analysis) for the starting point\n",
    "\n",
    "import codecs\n",
    "import protos.osmosis.gamm.pool_models.balancer.balancerPool_pb2\n",
    "import protos.osmosis.gamm.v1beta1.query_pb2 as query_gamms\n",
    "import protos.cosmos.bank.v1beta1.query_pb2 as query_bank\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "\n",
    "def _send_abci_query(request_msg, path, response_msg, height):\n",
    "    \"\"\"Encode and send pre-filled protobuf msg to RPC endpoint.\"\"\"\n",
    "    # Some queries have no data to pass.\n",
    "    if request_msg:\n",
    "        request_msg = codecs.encode(request_msg.SerializeToString(), 'hex')\n",
    "        request_msg = str(request_msg, 'utf-8')\n",
    "\n",
    "    req = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"id\": \"1\",\n",
    "        \"method\": \"abci_query\",\n",
    "        \"params\": {\n",
    "            \"height\": str(height),\n",
    "            \"path\": path,\n",
    "            \"data\": request_msg\n",
    "        }\n",
    "    }\n",
    "    req = json.dumps(req)\n",
    "    resp = requests.post(RPC_ENDPOINT, req, timeout=5).json()\n",
    "    if 'result' not in resp:\n",
    "        print(resp)\n",
    "    response = resp['result']['response']['value']\n",
    "    if not response:\n",
    "        print(resp)\n",
    "    response = base64.b64decode(response)\n",
    "    result = response_msg()\n",
    "    result.ParseFromString(response)\n",
    "    result = MessageToDict(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_onchain_pool_data(height):\n",
    "    request_msg = query_gamms.QueryPoolsRequest()\n",
    "    request_msg.pagination.limit = 10000\n",
    "    response_msg = query_gamms.QueryPoolsResponse\n",
    "    done = False\n",
    "    while not done:\n",
    "        try:\n",
    "            pool_data = _send_abci_query(request_msg=request_msg,\n",
    "                                        path=\"/osmosis.gamm.v1beta1.Query/Pools\",\n",
    "                                        response_msg=response_msg,\n",
    "                                        height=height)\n",
    "            done = True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"error while fetching pools data for height {}. Retrying...\".format(height))\n",
    "\n",
    "    pool_map = {}\n",
    "    for pool in pool_data.get('pools'):\n",
    "        pool_map.update({int(pool.get('id')): pool})\n",
    "\n",
    "    return pool_map\n",
    "\n",
    "\n",
    "def get_onchain_balance(height, address, denom):\n",
    "    request_msg = query_bank.QueryBalanceRequest()\n",
    "    request_msg.address = address\n",
    "    request_msg.denom = denom\n",
    "    response_msg = query_bank.QueryBalanceResponse\n",
    "    done = False\n",
    "    while not done:\n",
    "        try:\n",
    "            balance = _send_abci_query(request_msg=request_msg,\n",
    "                                        path=\"/cosmos.bank.v1beta1.Query/Balance\",\n",
    "                                        response_msg=response_msg,\n",
    "                                        height=height)\n",
    "            done = True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"error while fetching balance data for address {} at height {}. Retrying...\".format(address, height))\n",
    "\n",
    "    return int(balance[\"balance\"][\"amount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define functions to perform join/exit computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to compute correct amounts when doing join/exit\n",
    "# uses fixed point arithmetic for a better approximation of `sdk.Dec`\n",
    "# the logic here is copied 1~=1 from Osmosis\n",
    " \n",
    "from decimal import Context, Decimal, getcontext, ROUND_DOWN, ROUND_CEILING, ROUND_FLOOR\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "getcontext().prec = 18\n",
    "getcontext().rounding = ROUND_DOWN\n",
    "\n",
    "\n",
    "def pool_balance_of_denom(denom, pool):\n",
    "    pool_assets = pool[\"poolAssets\"]\n",
    "    for pool_asset in pool_assets:\n",
    "        if pool_asset[\"token\"][\"denom\"] == denom:\n",
    "            return Decimal(pool_asset[\"token\"][\"amount\"])\n",
    "    raise Exception(\"cannot find denom {} in pool {}\".format(denom, pool[\"id\"]))\n",
    "\n",
    "\n",
    "def pool_asset_of_denom(denom, pool):\n",
    "    pool_assets = pool[\"poolAssets\"]\n",
    "    for pool_asset in pool_assets:\n",
    "        if pool_asset[\"token\"][\"denom\"] == denom:\n",
    "            return pool_asset\n",
    "    raise Exception(\"cannot find denom {} in pool {}\".format(denom, pool[\"id\"]))\n",
    "\n",
    "\n",
    "def solve_constant_func_invariant(amt_x_before, amt_x_after, amt_x_weight, amt_y_before, amt_y_weight):\n",
    "\tweight_ratio = Decimal(amt_x_weight) / Decimal(amt_y_weight)\n",
    "\ty = Decimal(amt_x_before) / Decimal(amt_x_after)\n",
    "\treturn Decimal(amt_y_before) * (Decimal(1) - (y ** weight_ratio))\n",
    "\n",
    "\n",
    "# defines logic to compute the correct amount of shares when performing a join\n",
    "def correct_join_pool(tokens_in, pool):\n",
    "    # MaximalExactRatioJoin\n",
    "    min_share_ratio = Decimal('Infinity')\n",
    "    max_share_ratio = Decimal(0)\n",
    "    coin_share_ratios = dict()\n",
    "    for coin in tokens_in:\n",
    "        amt_in = Decimal(coin[\"amount\"])\n",
    "        share_ratio = amt_in / pool_balance_of_denom(coin[\"denom\"], pool)\n",
    "        if share_ratio < min_share_ratio:\n",
    "            min_share_ratio = share_ratio\n",
    "        if share_ratio > max_share_ratio:\n",
    "            max_share_ratio = share_ratio\n",
    "        coin_share_ratios[coin[\"denom\"]] = share_ratio\n",
    "    num_shares = int((min_share_ratio * Decimal(pool[\"totalShares\"][\"amount\"])).to_integral_value(rounding=ROUND_FLOOR))\n",
    "    rem_coins = list()\n",
    "    added_coins = list()\n",
    "    if min_share_ratio != max_share_ratio:\n",
    "        for coin in tokens_in:\n",
    "            if coin_share_ratios[coin[\"denom\"]] == min_share_ratio:\n",
    "                added_coins.append(coin)\n",
    "                continue\n",
    "        used_amt = Context(rounding=ROUND_CEILING).quantize(min_share_ratio * pool_balance_of_denom(coin[\"denom\"], pool), 0)\n",
    "        new_amt = int(coin[\"amount\"]) - int(used_amt)\n",
    "        added_coins.append({\"denom\": coin[\"denom\"], \"amount\": used_amt})\n",
    "        if new_amt > 0:\n",
    "            rem_coins.append({\"denom\": coin[\"denom\"], \"amount\": new_amt})\n",
    "\n",
    "    updated_pool = deepcopy(pool)\n",
    "    updated_pool[\"totalShares\"][\"amount\"] = int(updated_pool[\"totalShares\"][\"amount\"]) +  num_shares\n",
    "    for coin in added_coins:\n",
    "        for pool_asset in updated_pool[\"poolAssets\"]:\n",
    "            if pool_asset[\"token\"][\"denom\"] == coin[\"denom\"]:\n",
    "                pool_asset[\"token\"][\"amount\"] = int(pool_asset[\"token\"][\"amount\"]) + int(coin[\"amount\"])\n",
    "\n",
    "    # calcJoinSingleAssetTokensIn\n",
    "    for coin in rem_coins:\n",
    "        pool_asset = pool_asset_of_denom(coin[\"denom\"], updated_pool)\n",
    "        pool_balance = Decimal(pool_asset[\"token\"][\"amount\"])\n",
    "        normalized_weight = Decimal(pool_asset[\"weight\"]) / Decimal(pool[\"totalWeight\"])\n",
    "        swap_fee = pool[\"poolParams\"][\"swapFee\"]\n",
    "        swap_fee = Decimal(\"0.\" + \"0\" * (18 - len(swap_fee)) + swap_fee)\n",
    "        token_amt_after_fee = Decimal(coin[\"amount\"]) * (Decimal(1) - (Decimal(1) - normalized_weight) * swap_fee)\n",
    "        new_shares = -solve_constant_func_invariant(pool_balance + token_amt_after_fee, pool_balance, normalized_weight, updated_pool[\"totalShares\"][\"amount\"], 1)\n",
    "        num_shares += int(new_shares.to_integral_value(rounding=ROUND_FLOOR))\n",
    "\n",
    "    return num_shares\n",
    "\n",
    "\n",
    "# compute liquidity removed as a result of burning the provided amount of shares. Used to compute shares on corrected pool history.\n",
    "def burn_shares(pool, shares_burned_amt):\n",
    "    shares_burned_amt = Decimal(shares_burned_amt)\n",
    "    exit_fee = pool[\"poolParams\"][\"exitFee\"]\n",
    "    exit_fee = Decimal(\"0.\" + \"0\" * (18 - len(exit_fee)) + exit_fee)\n",
    "    if exit_fee > 0:\n",
    "        shares = shares_burned_amt * (Decimal(1) - exit_fee)\n",
    "    else:\n",
    "        shares = shares_burned_amt\n",
    "    share_out_ratio = shares / Decimal(pool[\"totalShares\"][\"amount\"])\n",
    "    tokens_out = list()\n",
    "    for pool_asset in pool[\"poolAssets\"]:\n",
    "        amt_out = (share_out_ratio * Decimal(pool_asset[\"token\"][\"amount\"])).to_integral_value(rounding=ROUND_FLOOR)\n",
    "        tokens_out.append({\"denom\": pool_asset[\"token\"][\"denom\"], \"amount\": int(amt_out)}) \n",
    "    return tokens_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define helper functions to handle coins and pool assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick and dirty function to split a normalized coin string into denom and amount\n",
    "def parse_token_str(token_str):\n",
    "    denom_index_start = token_str.find(next(filter(str.isalpha, token_str)))\n",
    "    amount, denom = int(token_str[:denom_index_start]), token_str[denom_index_start:]\n",
    "    return {\"amount\": amount, \"denom\": denom}\n",
    "\n",
    "\n",
    "# another quick and dirty function to perform `sdk.Coins.AddCoins()`\n",
    "def sub_coins(a, b):\n",
    "    tolerance = 100 # small tolerance for subtraction. Values could slightly diverge due to approximation.\n",
    "    res = list()\n",
    "    for coinb in b:\n",
    "        for coina in a:\n",
    "            if coina[\"denom\"] == coinb[\"denom\"]:\n",
    "                if int(coina[\"amount\"]) + tolerance < int(coinb[\"amount\"]):\n",
    "                    raise Exception(\"would result in coin with negative amount\")\n",
    "                amt = int(coina[\"amount\"]) - int(coinb[\"amount\"])\n",
    "                amt = 0 if amt < 0 else amt # clamp (< tolerance) negative values to 0\n",
    "                if amt != 0:\n",
    "                    # only add entry if amount is not 0\n",
    "                    res.append({\"denom\": coina[\"denom\"], \"amount\": amt})\n",
    "                break\n",
    "        else:\n",
    "            raise Exception(\"Unable to find denom {} in primary coin list\".format(coinb[\"denom\"]))\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "# update the pool assets using the provided lists of tokens in and tokens out\n",
    "def update_pool_assets(tokens_in, tokens_out, pool):\n",
    "    pool_assets = pool[\"poolAssets\"]\n",
    "    for token_in in tokens_in:\n",
    "        for pool_asset in pool_assets:\n",
    "            if pool_asset[\"token\"][\"denom\"] == token_in[\"denom\"]:\n",
    "               pool_asset[\"token\"][\"amount\"] = str(int(pool_asset[\"token\"][\"amount\"]) + int(token_in[\"amount\"]))\n",
    "               break\n",
    "        else:\n",
    "            raise Exception(\"cannot find asset {} in pool {}\".format(token_in[\"denom\"], pool[\"id\"]))\n",
    "    for token_out in tokens_out:\n",
    "        for pool_asset in pool_assets:\n",
    "            if pool_asset[\"token\"][\"denom\"] == token_out[\"denom\"]:\n",
    "               pool_asset[\"token\"][\"amount\"] = str(int(pool_asset[\"token\"][\"amount\"]) - int(token_out[\"amount\"]))\n",
    "               break\n",
    "        else:\n",
    "            raise Exception(\"cannot find asset {} in pool {}\".format(token_out[\"denom\"], pool[\"id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define main txs processing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a tx on both \"proper\" and \"corrupted\" pools. The \"corrupted pools\" data only use real block data (also for joins) and is used for consistency checks with onchain data\n",
    "def apply_tx(tx, proper_pools, corrupted_pools, affected_wallets):\n",
    "    msg_type = tx[\"@type\"]\n",
    "    tx_height = int(tx[\"height\"])\n",
    "    \n",
    "    if \"MsgSwapExactAmountIn\" in msg_type or \"MsgSwapExactAmountOut\" in msg_type:\n",
    "        pool_id = int(tx[\"poolId\"])\n",
    "        tokens_in = [parse_token_str(tx[\"tokensIn\"])] # can only be one token\n",
    "        tokens_out = [parse_token_str(tx[\"tokensOut\"])] # can only be one token\n",
    "        update_pool_assets(tokens_in, tokens_out, proper_pools[pool_id])\n",
    "        update_pool_assets(tokens_in, tokens_out, corrupted_pools[pool_id])\n",
    "\n",
    "    elif \"MsgJoinSwapExternAmountIn\" in msg_type or \"MsgJoinSwapShareAmountOut\" in msg_type:\n",
    "        pool_id = int(tx[\"poolId\"])\n",
    "        tokens_in = [parse_token_str(t) for t in tx[\"tokensIn\"].split(\",\")]\n",
    "        update_pool_assets(tokens_in, [], proper_pools[pool_id])\n",
    "        update_pool_assets(tokens_in, [], corrupted_pools[pool_id])\n",
    "        shares_added = parse_token_str(tx[\"tokensOut\"])\n",
    "        if proper_pools[pool_id][\"totalShares\"][\"denom\"] != shares_added[\"denom\"]:\n",
    "           raise Exception(\"invalid shares minted for tx type {} hash {} height\".format(tx[\"@type\"], tx[\"txhash\"], tx_height)) \n",
    "        proper_pools[pool_id][\"totalShares\"][\"amount\"] = str(int(proper_pools[pool_id][\"totalShares\"][\"amount\"]) + int(shares_added[\"amount\"]))\n",
    "        corrupted_pools[pool_id][\"totalShares\"][\"amount\"] = str(int(corrupted_pools[pool_id][\"totalShares\"][\"amount\"]) + int(shares_added[\"amount\"]))\n",
    "    \n",
    "    elif \"MsgExitSwapExternAmountOut\" in msg_type or \"MsgExitSwapShareAmountIn\" in msg_type:\n",
    "        raise Exception(\"not implemented - should not be needed\")\n",
    "\n",
    "    elif \"MsgExitPool\" in msg_type or \"MsgUnPoolWhitelistedPool\" in msg_type:\n",
    "        addr = tx[\"sender\"]\n",
    "        pool_id = int(tx[\"poolId\"])\n",
    "        shares_burned = parse_token_str(tx[\"tokensIn\"])\n",
    "        if proper_pools[pool_id][\"totalShares\"][\"denom\"] != shares_burned[\"denom\"]:\n",
    "           raise Exception(\"invalid shares minted for tx type {} hash {} height\".format(tx[\"@type\"], tx[\"txhash\"], tx_height)) \n",
    "        tokens_out = [parse_token_str(t) for t in tx[\"tokensOut\"].split(\",\")]\n",
    "        if addr in affected_wallets and pool_id in affected_wallets[addr] and \"shares_inflated\" in affected_wallets[addr][pool_id]:\n",
    "            only_valid_shares = False\n",
    "            # if entry is in `affected_wallets`, the account performed a join earlier\n",
    "            shares_inflated_amt = affected_wallets[addr][pool_id][\"shares_inflated\"]\n",
    "            shares_valid_amt = affected_wallets[addr][pool_id][\"shares_valid\"]\n",
    "            if shares_burned[\"amount\"] > shares_valid_amt + shares_inflated_amt:\n",
    "                # burning more shares than what was minted post-upgrade, hence using shares created prior to the upgrade.\n",
    "                # we check the outstanding shares balance of the account at the previous height (querying onchain data) to see if the account\n",
    "                # had enough shares (minus the inflated) to cover for the exit.\n",
    "                # In such a case, we use valid shares and leave the inflated shres in the outstanding balance.\n",
    "                # The user will just have a LP shares debt (that will have to be burned) but not a liquidity debt.\n",
    "                # We do this only here so we limit to querying for balance only if strictly needed.\n",
    "                shares_balance_amt = get_onchain_balance(tx_height-1, addr, shares_burned[\"denom\"])\n",
    "                if shares_burned[\"amount\"] > shares_balance_amt:\n",
    "                    raise Exception(\"this should not happen\")\n",
    "                shares_valid_amt = shares_balance_amt - shares_inflated_amt \n",
    "                if shares_valid_amt > shares_burned[\"amount\"]:\n",
    "                    # clamp to a max of shares_burned[\"amount\"]\n",
    "                    shares_valid_amt = shares_burned[\"amount\"]\n",
    "            if shares_burned[\"amount\"] >= shares_valid_amt:\n",
    "                # update outstanding shares in `affected_wallets`\n",
    "                affected_wallets[addr][pool_id][\"shares_inflated\"] = shares_inflated_amt + shares_valid_amt - shares_burned[\"amount\"]\n",
    "                affected_wallets[addr][pool_id][\"shares_valid\"] = 0\n",
    "            else:\n",
    "                # in case the shares burned are less than the valid shares available, use those first (if inflated shares are never claimed,\n",
    "                # they can simply be burned)\n",
    "                only_valid_shares = True\n",
    "                shares_valid_amt = shares_burned[\"amount\"]\n",
    "                affected_wallets[addr][pool_id][\"shares_valid\"] -= shares_valid_amt\n",
    "            # simulate burning correct number of shares\n",
    "            correct_tokens_out = burn_shares(proper_pools[pool_id], shares_valid_amt)\n",
    "            if only_valid_shares:\n",
    "                # the tx is using only valid shares, so we will compute a refund.\n",
    "                # If the wallet claims the inflated shares in the future, the net will be a debt and not a credit towards the pool\n",
    "                diff = sub_coins(correct_tokens_out, tokens_out)\n",
    "                if diff: \n",
    "                   affected_wallets[addr][pool_id][\"tokens_owed\"] = diff \n",
    "            else:\n",
    "                # add entry for stolen amount\n",
    "                affected_wallets[addr][pool_id][\"tokens_stolen\"] = sub_coins(tokens_out, correct_tokens_out)\n",
    "        else:\n",
    "            # this exit does not happen after a v9 join, so the account is possibly due a refund\n",
    "            shares_valid_amt = shares_burned[\"amount\"]\n",
    "            correct_tokens_out = burn_shares(proper_pools[pool_id], shares_valid_amt)\n",
    "            diff = sub_coins(correct_tokens_out, tokens_out)\n",
    "            if diff:\n",
    "                if addr not in affected_wallets:\n",
    "                    affected_wallets[addr] = {pool_id: {}}\n",
    "                elif pool_id not in affected_wallets[addr]:\n",
    "                    affected_wallets[addr][pool_id] = {}\n",
    "                affected_wallets[addr][pool_id][\"tokens_owed\"] = diff\n",
    "        # update corrected pool with valid data\n",
    "        update_pool_assets([], correct_tokens_out, proper_pools[pool_id])\n",
    "        proper_pools[pool_id][\"totalShares\"][\"amount\"] = str(int(proper_pools[pool_id][\"totalShares\"][\"amount\"]) - int(shares_valid_amt))\n",
    "        # update pool used for consistency check with execution data\n",
    "        update_pool_assets([], tokens_out, corrupted_pools[pool_id])\n",
    "        corrupted_pools[pool_id][\"totalShares\"][\"amount\"] = str(int(corrupted_pools[pool_id][\"totalShares\"][\"amount\"]) - int(shares_burned[\"amount\"]))\n",
    "\n",
    "    elif \"MsgJoinPool\" in msg_type:\n",
    "        addr = tx[\"sender\"]\n",
    "        pool_id = int(tx[\"poolId\"])\n",
    "        tokens_in = [parse_token_str(t) for t in tx[\"tokensIn\"].split(\",\")]\n",
    "        shares_added = parse_token_str(tx[\"tokensOut\"])\n",
    "        if proper_pools[pool_id][\"totalShares\"][\"denom\"] != shares_added[\"denom\"]:\n",
    "           raise Exception(\"invalid shares minted for tx type {} hash {} height\".format(tx[\"@type\"], tx[\"txhash\"], tx[\"height\"]))\n",
    "        # update correct pool data with proper shares amount, and use exec data to update consistency-check pool\n",
    "        corrupted_pools[pool_id][\"totalShares\"][\"amount\"] = str(int(corrupted_pools[pool_id][\"totalShares\"][\"amount\"]) + int(shares_added[\"amount\"]))\n",
    "        correct_shares_amt = correct_join_pool(tokens_in, proper_pools[pool_id])\n",
    "        proper_pools[pool_id][\"totalShares\"][\"amount\"] = str(int(proper_pools[pool_id][\"totalShares\"][\"amount\"]) + int(correct_shares_amt))\n",
    "        update_pool_assets(tokens_in, [], proper_pools[pool_id])\n",
    "        update_pool_assets(tokens_in, [], corrupted_pools[pool_id])\n",
    "        # add entry for wallet-pool to track creation of inflated shares\n",
    "        if addr not in affected_wallets:\n",
    "            affected_wallets[addr] = {pool_id: {\"shares_valid\": 0, \"shares_inflated\": 0}}\n",
    "        elif pool_id not in affected_wallets[addr]:\n",
    "            affected_wallets[addr][pool_id] = {\"shares_valid\": 0, \"shares_inflated\": 0}\n",
    "        elif \"shares_valid\" not in affected_wallets[addr][pool_id]:\n",
    "            affected_wallets[addr][pool_id][\"shares_valid\"] = 0\n",
    "            affected_wallets[addr][pool_id][\"shares_inflated\"] = 0  \n",
    "        affected_wallets[addr][pool_id][\"shares_valid\"] += correct_shares_amt\n",
    "        affected_wallets[addr][pool_id][\"shares_inflated\"] += shares_added[\"amount\"] - correct_shares_amt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run analysis\n",
    "\n",
    "this is done sequentially, row-by-row. It can take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query initial pools data (before upgrade takes effect)\n",
    "initial_pools_data = get_onchain_pool_data(UPGRADE_HEIGHT-1)\n",
    "# update swap fees on pool 1 (happened at update time)\n",
    "initial_pools_data[1][\"poolParams\"][\"swapFee\"] = \"2000000000000000\"\n",
    "\n",
    "corrected_pools_history = list()\n",
    "affected_wallets = dict()\n",
    "proper_pools = initial_pools_data\n",
    "corrupted_pools = deepcopy(initial_pools_data) # this is to compare with onchain data for consistency\n",
    "print(\"processing block:\", UPGRADE_HEIGHT, \"remaining:\", \"{:5d}\".format(HALT_HEIGHT - UPGRADE_HEIGHT), end='\\r')\n",
    "last_index = len(df) - 1\n",
    "for index, tx in df.iterrows():\n",
    "    # apply tx to update data structures\n",
    "    apply_tx(tx, proper_pools, corrupted_pools, affected_wallets)\n",
    "    # get netx tx height\n",
    "    curr_height = int(tx[\"height\"])\n",
    "    if index == last_index:\n",
    "        print(\"\\ndone\")\n",
    "        continue # skip rest of the code as it would not be valid for the last element\n",
    "    next_height = int(df.loc[index+1][\"height\"])\n",
    "    if curr_height < next_height:\n",
    "        # next tx is in a new block, finalize pools for this height\n",
    "        # (these check could be disabled to speedup calculation)\n",
    "        corrected_pools_history.append(deepcopy(proper_pools))\n",
    "        onchain_pools_data = get_onchain_pool_data(curr_height)\n",
    "        if onchain_pools_data != corrupted_pools:\n",
    "            raise Exception(\"inconsistency found between simulated and onchain data for pools at height {}\".format(curr_height))\n",
    "        print(\"processing block:\", next_height, \"remaining:\", \"{:5d}\".format(HALT_HEIGHT - next_height), end='\\r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process retults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_str(tokens_list):\n",
    "    if type(tokens_list) is not list:\n",
    "        return \"\"\n",
    "    return \",\".join([str(token[\"amount\"]) + token[\"denom\"] for token in tokens_list])\n",
    "\n",
    "def net_difference_tokens(row):\n",
    "    stolen = row[3]\n",
    "    owed = row[4]\n",
    "    if stolen and owed:\n",
    "        # we assume stolen > owed\n",
    "        stolen = sub_coins(stolen, owed)\n",
    "        owed = \"\"\n",
    "    return stolen, owed\n",
    "\n",
    "# generate pandas DataFrame\n",
    "wallets_df = pd.DataFrame.from_dict([[addr, pool_id, pool_data] for addr, addr_pools in affected_wallets.items() for pool_id, pool_data in addr_pools.items()])\n",
    "wallets_df.columns = [\"address\", \"pool_id\", \"pool_data\"]\n",
    "\n",
    "# expand pool data\n",
    "wallets_df = wallets_df.join(pd.json_normalize(wallets_df[\"pool_data\"])[[\"shares_inflated\", \"tokens_stolen\", \"tokens_owed\"]]).drop(columns=[\"pool_data\"])\n",
    "\n",
    "# compute net difference if entries in both `tokens_stolen` and tokens_owned\n",
    "wallets_df[\"tokens_stolen\"], wallets_df[\"tokens_owed\"] = zip(*wallets_df.apply(net_difference_tokens, axis=1))\n",
    "\n",
    "# convert tokens list into strings\n",
    "wallets_df.tokens_owed = wallets_df.tokens_owed.apply(token_to_str)\n",
    "wallets_df.tokens_stolen = wallets_df.tokens_stolen.apply(token_to_str)\n",
    "\n",
    "wallets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save wallets info as `wallets.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallets_df.to_csv(\"csv/wallets.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transactions occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"@type\").size().reset_index(name=\"number_of_txs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group transactions by sender and sort by number of txs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupby([\"@type\", \"sender\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"number_of_txs\")\n",
    "    .sort_values(by=[\"number_of_txs\"], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find wallets-by-pools that did either join or exit. Count occurrences of joins/exits per pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[(df[\"@type\"].str.contains(\"MsgJoinPool\")) | (df[\"@type\"].str.contains(\"MsgExitPool\"))]\n",
    "    .groupby([\"@type\", \"sender\", \"poolId\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"number_of_txs\")\n",
    "    .sort_values(by=[\"number_of_txs\"], ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
